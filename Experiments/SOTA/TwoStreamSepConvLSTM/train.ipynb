{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "from numpy.random import seed, shuffle\n",
    "from random import seed as rseed\n",
    "from tensorflow.random import set_seed\n",
    "seed(42)\n",
    "rseed(42)\n",
    "set_seed(42)\n",
    "# import random\n",
    "# import pickle\n",
    "import shutil\n",
    "import models\n",
    "from utils import *\n",
    "from dataGenerator import *\n",
    "from datasetProcess import *\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam # RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler # EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.python.keras import backend as K\n",
    "# import pandas as pd\n",
    "import argparse\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    mode = args.mode # [\"both\",\"only_frames\",\"only_differences\"]\n",
    "\n",
    "    if args.fusionType != 'C':\n",
    "        if args.mode != 'both':\n",
    "            print(\"Only Concat fusion supports one stream versions. Changing mode to /'both/'...\")\n",
    "            mode = \"both\"\n",
    "        if args.lstmType == '3dconvblock':\n",
    "            raise Exception('3dconvblock instead of lstm is only available for fusionType C ! aborting execution...')\n",
    "\n",
    "    if args.fusionType == 'C': # default\n",
    "        model_function = models.getProposedModelC\n",
    "    elif args.fusionType == 'A':\n",
    "        model_function = models.getProposedModelA\n",
    "    elif args.fusionType == 'M':\n",
    "        model_function = models.getProposedModelM\n",
    "\n",
    "    dataset = args.dataset # ['rwf2000','movies','hockey']\n",
    "    dataset_videos = {'hockey':'raw_videos/HockeyFights','movies':'raw_videos/movies'}\n",
    "\n",
    "    if dataset == \"rwf2000\":\n",
    "        initial_learning_rate = 4e-04\n",
    "    elif dataset == \"hockey\":\n",
    "        initial_learning_rate = 1e-06 \n",
    "    elif dataset == \"movies\":\n",
    "        initial_learning_rate = 1e-05 \n",
    "\n",
    "    batch_size = args.batchSize # 4\n",
    "\n",
    "    vid_len = args.vidLen  # 32\n",
    "    if dataset == \"rwf2000\":\n",
    "        dataset_frame_size = 320\n",
    "    else:\n",
    "        dataset_frame_size = 224\n",
    "    frame_diff_interval = 1\n",
    "    input_frame_size = 224 # 320\n",
    "\n",
    "    lstm_type = args.lstmType # attensepconv\n",
    "\n",
    "    crop_dark = {\n",
    "        'hockey' : (16,45),\n",
    "        'movies' : (18,48),\n",
    "        'rwf2000': (0,0)\n",
    "    }\n",
    "\n",
    "    #---------------------------------------------------\n",
    "\n",
    "    epochs = args.numEpochs\n",
    "    preprocess_data = args.preprocessData\n",
    "    create_new_model = ( not args.resume )\n",
    "    save_path = args.savePath\n",
    "    resume_path = args.resumePath\n",
    "    background_suppress = args.noBackgroundSuppression\n",
    "\n",
    "    if resume_path == \"NOT_SET\":\n",
    "        currentModelPath =  os.path.join(save_path , str(dataset) + '_currentModel')\n",
    "    else:\n",
    "        currentModelPath = resume_path\n",
    "\n",
    "    bestValPath =  os.path.join(save_path, str(dataset) + '_best_val_acc_Model')  \n",
    "\n",
    "    rwfPretrainedPath = args.rwfPretrainedPath\n",
    "    if rwfPretrainedPath == \"NOT_SET\": # default='NOT_SET'\n",
    "        \n",
    "        if lstm_type == \"sepconv\": # default='sepconv' and defined\n",
    "            ###########################\n",
    "            # rwfPretrainedPath contains path to the model which is already trained on rwf2000 dataset. It is used to initialize training on hockey or movies dataset\n",
    "            # get this model from the trained_models google drive folder that I provided in readme \n",
    "            ###########################\n",
    "            rwfPretrainedPath = \"./trained_models/rwf2000_model/sepconvlstm-M/model/rwf2000_model\"   # if you are using M model\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    resume_learning_rate = 5e-05   \n",
    "    cnn_trainable = True  \n",
    "    one_hot = False\n",
    "    loss = 'binary_crossentropy'\n",
    "\n",
    "    #----------------------------------------------------\n",
    "\n",
    "    if preprocess_data:\n",
    "        if dataset == 'rwf2000':\n",
    "            os.mkdir(os.path.join(dataset, 'processed'))\n",
    "            convert_dataset_to_npy(src=f'{dataset}/RWF-2000', dest=f'{dataset}/processed', crop_x_y=None, target_frames=vid_len, frame_size= dataset_frame_size)\n",
    "        else:\n",
    "            if os.path.exists('{}'.format(dataset)):\n",
    "                shutil.rmtree('{}'.format(dataset))\n",
    "            split = train_test_split(dataset_name=dataset,source=dataset_videos[dataset])\n",
    "            os.mkdir(dataset)\n",
    "            os.mkdir(os.path.join(dataset,'videos'))\n",
    "            move_train_test(dest='{}/videos'.format(dataset),data=split)\n",
    "            os.mkdir(os.path.join(dataset,'processed'))\n",
    "            convert_dataset_to_npy(src='{}/videos'.format(dataset),dest='{}/processed'.format(dataset), crop_x_y=crop_dark[dataset], target_frames=vid_len, frame_size= dataset_frame_size )\n",
    "\n",
    "    train_generator = DataGenerator(directory = f'{dataset}/processed/train',\n",
    "                                    batch_size = batch_size,\n",
    "                                    data_augmentation = False,\n",
    "                                    shuffle = True,\n",
    "                                    one_hot = one_hot,\n",
    "                                    sample = False,\n",
    "                                    resize = input_frame_size,\n",
    "                                    background_suppress = background_suppress,\n",
    "                                    target_frames = vid_len,\n",
    "                                    dataset = dataset,\n",
    "                                    mode = mode)\n",
    "    # print(train_generator)\n",
    "\n",
    "    test_generator = DataGenerator(directory = f'{dataset}/processed/test',\n",
    "                                    batch_size = batch_size,\n",
    "                                    data_augmentation = False,\n",
    "                                    shuffle = False,\n",
    "                                    one_hot = one_hot,\n",
    "                                    sample = False,\n",
    "                                    resize = input_frame_size,\n",
    "                                    background_suppress = background_suppress,\n",
    "                                    target_frames = vid_len,\n",
    "                                    dataset = dataset,\n",
    "                                    mode = mode)\n",
    "\n",
    "    #--------------------------------------------------\n",
    "\n",
    "    print('> cnn_trainable : ',cnn_trainable)\n",
    "    if create_new_model:\n",
    "        print('> creating new model...')\n",
    "        model = model_function(size=input_frame_size, seq_len=vid_len,cnn_trainable=cnn_trainable, frame_diff_interval = frame_diff_interval, mode=mode, lstm_type=lstm_type)\n",
    "        if dataset == \"hockey\" or dataset == \"movies\":\n",
    "            print('> loading weights pretrained on rwf dataset from', rwfPretrainedPath)\n",
    "            model.load_weights(rwfPretrainedPath)\n",
    "        optimizer = Adam(lr=initial_learning_rate, amsgrad=True)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "        print('> new model created')    \n",
    "    else: # this will work in seq\n",
    "        print('> getting the model from...', currentModelPath)  \n",
    "        if dataset == 'rwf2000':\n",
    "            model =  model_function(size=input_frame_size, seq_len=vid_len,cnn_trainable=cnn_trainable, frame_diff_interval = frame_diff_interval, mode=mode, lstm_type=lstm_type)\n",
    "            optimizer = Adam(lr=resume_learning_rate, amsgrad=True)\n",
    "            model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "            model.load_weights(f'{currentModelPath}')\n",
    "        elif  dataset == \"hockey\" or dataset == \"movies\":\n",
    "            model =  model_function(size=input_frame_size, seq_len=vid_len,cnn_trainable=cnn_trainable, frame_diff_interval = frame_diff_interval, mode=mode, lstm_type=lstm_type)\n",
    "            optimizer = Adam(lr=initial_learning_rate, amsgrad=True)\n",
    "            model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "            model.load_weights(f'{currentModelPath}')           \n",
    "\n",
    "    print('> Summary of the model : ')\n",
    "    model.summary(line_length=140)\n",
    "    print('> Optimizer : ', model.optimizer.get_config())\n",
    "\n",
    "    # dot_img_file = 'model_architecture.png'\n",
    "    # print('> plotting the model architecture and saving at ', dot_img_file)\n",
    "    # plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
    "\n",
    "    #--------------------------------------------------\n",
    "\n",
    "    modelcheckpoint = ModelCheckpoint(\n",
    "        currentModelPath, monitor='loss', verbose=0, save_best_only=False, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    modelcheckpointVal = ModelCheckpoint(\n",
    "        bestValPath, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "    historySavePath = os.path.join(save_path, 'results', str(dataset))\n",
    "    save_training_history = SaveTrainingCurves(save_path = historySavePath)\n",
    "\n",
    "    callback_list = [\n",
    "                    modelcheckpoint,\n",
    "                    modelcheckpointVal,\n",
    "                    save_training_history\n",
    "                    ]\n",
    "                    \n",
    "    callback_list.append(LearningRateScheduler(lr_scheduler, verbose = 0))\n",
    "                    \n",
    "    #--------------------------------------------------\n",
    "\n",
    "    history = model.fit(\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        x=train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=len(test_generator),\n",
    "        verbose=1,\n",
    "        workers=8,\n",
    "        max_queue_size=8,\n",
    "        use_multiprocessing=False,\n",
    "        callbacks= callback_list\n",
    "    )\n",
    "\n",
    "    #---------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--numEpochs', type=int, default=50, help='Number of epochs')\n",
    "    parser.add_argument('--vidLen', type=int, default=32, help='Number of frames in a clip')\n",
    "    parser.add_argument('--batchSize', type=int, default=4, help='Training batch size')\n",
    "    parser.add_argument('--resume', help='whether training should resume from the previous checkpoint',action='store_true')\n",
    "    parser.add_argument('--noBackgroundSuppression', help='whether to use background suppression on frames',action='store_false')\n",
    "    parser.add_argument('--preprocessData', help='whether need to preprocess data ( make npy file from video clips )',action='store_true')\n",
    "    parser.add_argument('--mode', type=str, default='both', help='model type - both, only_frames, only_differences', choices=['both', 'only_frames', 'only_differences']) \n",
    "    parser.add_argument('--dataset', type=str, default='rwf2000', help='dataset - rwf2000, movies, hockey', choices=['rwf2000','movies','hockey']) \n",
    "    parser.add_argument('--lstmType', type=str, default='sepconv', help='lstm - conv, sepconv, asepconv, 3dconvblock(use 3dconvblock instead of lstm)', choices=['sepconv','asepconv', 'conv', '3dconvblock'])\n",
    "    parser.add_argument('--fusionType', type=str, default='C', help='fusion type - A for add, M for multiply, C for concat', choices=['C','A','M']) \n",
    "    parser.add_argument('--savePath', type=str, default='/gdrive/My Drive/THESIS/Data', help='folder path to save the models')\n",
    "    parser.add_argument('--rwfPretrainedPath', type=str, default='NOT_SET', help='path to the weights pretrained on rwf dataset')\n",
    "    parser.add_argument('--resumePath', type=str, default='NOT_SET', help='path to the weights for resuming from previous checkpoint')\n",
    "    parser.add_argument('--resumeLearningRate', type=float, default=5e-05, help='learning rate to resume training from')\n",
    "    args = parser.parse_args()\n",
    "    train(args)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background suppression: True\n",
      "Found 200 files belonging to 2 classes.\n",
      "     fight :  0\n",
      "  nonFight :  1\n",
      "background suppression: True\n",
      "Found 90 files belonging to 2 classes.\n",
      "     fight :  0\n",
      "  nonFight :  1\n",
      "> cnn_trainable :  True\n",
      "> creating new model...\n",
      "cnn_trainable: True\n",
      "cnn dropout :  0.25\n",
      "dense dropout :  0.3\n",
      "lstm dropout : 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 12:37:12.142005: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-27 12:37:12.885429: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-27 12:37:15.385711: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ajayt\\OneDrive\\Desktop\\Main P\\other\\TwoStreamSepConvLSTM_ViolenceDetection\\train.py\", line 234, in <module>\n",
      "    main()\n",
      "  File \"c:\\Users\\ajayt\\OneDrive\\Desktop\\Main P\\other\\TwoStreamSepConvLSTM_ViolenceDetection\\train.py\", line 232, in main\n",
      "    train(args)\n",
      "  File \"c:\\Users\\ajayt\\OneDrive\\Desktop\\Main P\\other\\TwoStreamSepConvLSTM_ViolenceDetection\\train.py\", line 149, in train\n",
      "    model = model_function(size=input_frame_size, seq_len=vid_len,cnn_trainable=cnn_trainable, frame_diff_interval = frame_diff_interval, mode=mode, lstm_type=lstm_type)\n",
      "  File \"c:\\Users\\ajayt\\OneDrive\\Desktop\\Main P\\other\\TwoStreamSepConvLSTM_ViolenceDetection\\models.py\", line 49, in getProposedModelC\n",
      "    frames_cnn = Model( inputs=[frames_cnn.layers[0].input],outputs=[frames_cnn.layers[-30].output] ) # taking only upto block 13\n",
      "  File \"c:\\Users\\ajayt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\model.py\", line 143, in __new__\n",
      "    return functional.Functional(*args, **kwargs)\n",
      "  File \"c:\\Users\\ajayt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tracking.py\", line 25, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\ajayt\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 114, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: When providing `inputs` as a list/tuple, all values in the list/tuple must be KerasTensors. Received: inputs=[[]] including invalid value [] of type <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset rwf2000 --vidLen 32 --batchSize 4 --numEpochs 5 --mode both --lstmType sepconv --savePath \"C:/Users/ajayt/OneDrive/Desktop/Main P/other/saves\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
